{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #1\n",
    "\n",
    "For this question you will use [Olivetti Face Dataset](https://scikit-learn.org/0.19/datasets/olivetti_faces.html).\n",
    "\n",
    "### Part 1\n",
    "\n",
    "1. Split your dataset as train and test subset. But make sure that each test set contains exactly one random image from each distinct individual. This means, you will have to write your own train_test_split function for this dataset.\n",
    "\n",
    "2. Construct an SVM model on your train set, and test its accuracy on your test set. For this part, the images viewed as integer vectors of length 4096 are independent variables while the id number of the person that picture belongs to is the dependent variable. In other words, you are trying to construct an SVM model that recognizes individuals based on their pictures.\n",
    "\n",
    "3. Repeat Step 2 ten times.\n",
    "\n",
    "4. Calculate the mean accuracy and get 95% confidence interval using the t-test.\n",
    "\n",
    "### Part 2\n",
    "\n",
    "Do the same things you did in Part 1 but with a multinomial regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = fetch_olivetti_faces()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First write a train and test splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split():\n",
    "    test_index=[]\n",
    "    for i in range(40):\n",
    "        j=np.random.randint(0,10)\n",
    "        tmp=i*10+j\n",
    "        test_index.append(tmp)\n",
    "        \n",
    "    test_X=faces.data[test_index]\n",
    "    train_X=np.delete(faces.data,test_index,axis=0)\n",
    "    test_y=faces.target[test_index]\n",
    "    train_y=np.delete(faces.target,test_index)\n",
    "    return test_X,test_y,train_X,train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate model score 10 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.975, 0.975, 0.9, 0.975, 0.975, 0.95, 0.975, 0.975, 0.9, 0.95]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def svcmodel():\n",
    "    test_X,test_y,train_X,train_y=test_train_split(faces)\n",
    "    model=SVC()\n",
    "    model.fit(train_X,train_y)\n",
    "    sc=model.score(test_X,test_y)\n",
    "    \n",
    "    return sc\n",
    "\n",
    "scores=[svcmodel() for i in range(10)]\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets calcualte confidence interaval a=0.95 and mean of accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9549999999999998"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "u=np.mean(scores)\n",
    "sigma=np.std(scores)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8890473520273876, 1.020952647972612)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.interval(0.95,9,loc=u,scale=sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Desktop\\sample_project\\env\\lib\\site-packages\\scipy\\optimize\\linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "C:\\Users\\PC\\Desktop\\sample_project\\env\\lib\\site-packages\\sklearn\\utils\\optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "C:\\Users\\PC\\Desktop\\sample_project\\env\\lib\\site-packages\\scipy\\optimize\\linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "C:\\Users\\PC\\Desktop\\sample_project\\env\\lib\\site-packages\\sklearn\\utils\\optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "C:\\Users\\PC\\Desktop\\sample_project\\env\\lib\\site-packages\\scipy\\optimize\\linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "C:\\Users\\PC\\Desktop\\sample_project\\env\\lib\\site-packages\\sklearn\\utils\\optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "C:\\Users\\PC\\Desktop\\sample_project\\env\\lib\\site-packages\\scipy\\optimize\\linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "C:\\Users\\PC\\Desktop\\sample_project\\env\\lib\\site-packages\\sklearn\\utils\\optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6138888888888889,\n",
       " 0.625,\n",
       " 0.6833333333333333,\n",
       " 0.6166666666666667,\n",
       " 0.6166666666666667,\n",
       " 0.6388888888888888,\n",
       " 0.6194444444444445,\n",
       " 0.6111111111111112,\n",
       " 0.6027777777777777,\n",
       " 0.6138888888888889]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def LogRegressor():\n",
    "    test_X,test_y,train_X,train_y=test_train_split(faces)\n",
    "    lrmodel=LogisticRegression(solver=\"newton-cg\")\n",
    "    lrmodel.fit(test_X,test_y)\n",
    "    sc=lrmodel.score(train_X,train_y)\n",
    "    \n",
    "    return sc\n",
    "    \n",
    "scores=[LogRegressor() for i in range(10)]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6241666666666668"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "u=np.mean(scores)\n",
    "sigma=np.std(scores)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5751653471368569, 0.6731679861964767)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.interval(0.95,9,loc=u,scale=sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #2\n",
    "\n",
    "For this question you will use [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "### Part 1\n",
    "\n",
    "Convert the dataset into numerical data using [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) from SciKitLearn's `sklearn.feature_extraction.text` module. Make sure that you also record whether a given movie review is positive or negative or neutral. Calling on `CountVectorizer` on individual entries is not going to be enough. You will have to do some preliminary work. Read the documentation carefully.\n",
    "\n",
    "### Part 2\n",
    "\n",
    "Using the numerical data you constructed in Part 1, construct an LDA model to see if data projects into a 2D space with clear separation. Analyze your result.\n",
    "\n",
    "\n",
    "### Part 3\n",
    "\n",
    "Using the numerical data you constructed in Part 1, \n",
    "\n",
    "1. Split the data as train and test using SciKitLearn's `train_test_split` function.\n",
    "2. Form a multiclass SVM model on the train set and test its accuracy.\n",
    "3. Repeat a small number of times and get mean accuracy and its error band.\n",
    "\n",
    "### Part 4\n",
    "\n",
    "Repeat Part 2 using multinomial regression models instead of SVM.\n",
    "\n",
    "### Part 5\n",
    "\n",
    "Using the numerical data you constructed in Part 1, \n",
    "\n",
    "1. Construct an PCA model and look at the eigenvalues from largest to smallest. \n",
    "2. How many dimensions needed to capture 90% of the variation of the data? (Read the documentation of [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) form SciKitLearn)\n",
    "3. Transform your data using the result you obtained in Step 2.\n",
    "4. Construct an SVM model on the new dataset you constructed and cross-validate it.\n",
    "5. Compare your result with the result you obtained in Part 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the necessary functions have been written and required libraries are imported "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "from os import listdir\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "def read_file(name_folder):\n",
    "    file=open(name_folder,\"r\",encoding='utf-8')\n",
    "    text=file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def text_cleaner(text):\n",
    "    newString = text.lower()\n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    long_words=[]\n",
    "    for i in newString.split():\n",
    "        if len(i)>=3:                  \n",
    "            long_words.append(i)\n",
    "    return (\" \".join(long_words)).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pos_data and neg data are written to two diffrent list, to determine columns of total data these two lists are stacked together as all_data and text is vectorized.  Then transform our lists as pos and neg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory1=\"C:/Users/PC/Desktop/sample_project/statistics/Homework III/aclImdb/train/pos\"\n",
    "directory2=\"C:/Users/PC/Desktop/sample_project/statistics/Homework III/aclImdb/train/neg\"\n",
    "\n",
    "\n",
    "pos_data=[]\n",
    "neg_data=[]\n",
    "all_data=[]\n",
    "\n",
    "for filename in listdir(directory1):\n",
    "    path = directory1 + '/' + filename\n",
    "    tmp=read_file(path)\n",
    "    pos_data.append(tmp)\n",
    "    \n",
    "for filename in listdir(directory2):\n",
    "    path = directory2 + '/' + filename\n",
    "    tmp=read_file(path)\n",
    "    neg_data.append(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pos_data)):\n",
    "    pos_data[i]=text_cleaner(pos_data[i])\n",
    "\n",
    "\n",
    "for i in range(len(neg_data)):\n",
    "    neg_data[i]=text_cleaner(neg_data[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was too big to stacked, so each list is shuffled and selected 1/10 of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(pos_data)\n",
    "random.shuffle(neg_data)\n",
    "\n",
    "pos_data=pos_data[0:len(pos_data)//10]\n",
    "neg_data=neg_data[0:len(neg_data)//10]\n",
    "\n",
    "all_data=pos_data+neg_data\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(all_data)\n",
    "\n",
    "pos=vectorizer.transform(pos_data)\n",
    "neg=vectorizer.transform(neg_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while Positive=1, Negative=0, classes of each vector is appended to vectorized texts. And then they added end to end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_array=pos.toarray()\n",
    "neg_array=neg.toarray()\n",
    "class1=np.ones((len(pos_array),1))\n",
    "class0=np.zeros((len(neg_array),1))\n",
    "pos_array=np.append(pos_array,class1,axis=1)\n",
    "neg_array=np.append(neg_array,class0,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.append(pos_array,neg_array,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2\n",
    "\n",
    "\n",
    "Lda fit and transform is used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "lda=LinearDiscriminantAnalysis()\n",
    "X=data[:,0:(data.shape[1]-1)]\n",
    "y=data[:,data.shape[1]-1] \n",
    "\n",
    "a=lda.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9996"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=lda.predict(X)\n",
    "lda.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lda is able to seperate %99 of data,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1bc98875910>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(a,y_pred,c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM is applied 5 times, and mean score is calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8272, 0.7872, 0.7952, 0.7824, 0.7872]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "def apply_svm(X,y):\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25)\n",
    "    model=SVC()\n",
    "    model.fit(X_train,y_train)\n",
    "    sc=model.score(X_test,y_test)\n",
    "    return sc\n",
    "    \n",
    "scs=[]\n",
    "scs=[apply_svm(X,y) for i in range(5)]\n",
    "scs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79584"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4\n",
    "\n",
    "Logistic regression is applied 10 times and mean score is calculated, solver is selected as \"sag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Desktop\\sample_project\\env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\PC\\Desktop\\sample_project\\env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\PC\\Desktop\\sample_project\\env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\PC\\Desktop\\sample_project\\env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\PC\\Desktop\\sample_project\\env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\PC\\Desktop\\sample_project\\env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\PC\\Desktop\\sample_project\\env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\PC\\Desktop\\sample_project\\env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\PC\\Desktop\\sample_project\\env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\PC\\Desktop\\sample_project\\env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def Log_Reg(X,y):\n",
    "    \n",
    "    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25)\n",
    "    model=LogisticRegression(solver=\"sag\")\n",
    "    model.fit(X_train,y_train)\n",
    "    sc=model.score(X_test,y_test)\n",
    "    return sc\n",
    "\n",
    "scs=[Log_Reg(X,y) for i in range(10)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.789423076923077"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean score of log_reg is 0.789423076923077\n",
    "\n",
    "\n",
    "\n",
    "Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "explained=0\n",
    "i=250\n",
    "while(explained<0.9):\n",
    "    pca=PCA(n_components=i)\n",
    "    pca.fit(X,y)\n",
    "    explained=np.sum(pca.explained_variance_ratio_)\n",
    "    i+=1\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9001027535737368"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca=PCA(n_components=286)\n",
    "pca.fit(X,y)\n",
    "explained=np.sum(pca.explained_variance_ratio_)\n",
    "explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when n_component is 286, explanied variance is 0.900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca=pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data is transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77844311, 0.69461078, 0.69277108, 0.75301205, 0.74698795])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC \n",
    "clf = SVC()\n",
    "scores = cross_val_score(clf, X_pca, y, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
